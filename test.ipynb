{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean,std)]\n",
    ")\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x1551cf790>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 24, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        # x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = torch.flatten(x,1)\n",
    "        # x = x.view(-1, 16 * 5 * 5)            \n",
    "        x = F.relu(self.fc1(x))               \n",
    "        x = F.relu(self.fc2(x))               \n",
    "        x = self.fc3(x)                       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [64/782], Loss: 4.1493\n",
      "Epoch [1/50], Step [128/782], Loss: 4.1956\n",
      "Epoch [1/50], Step [192/782], Loss: 4.2641\n",
      "Epoch [1/50], Step [256/782], Loss: 4.0086\n",
      "Epoch [1/50], Step [320/782], Loss: 3.7989\n",
      "Epoch [1/50], Step [384/782], Loss: 3.7207\n",
      "Epoch [1/50], Step [448/782], Loss: 3.6891\n",
      "Epoch [1/50], Step [512/782], Loss: 3.5535\n",
      "Epoch [1/50], Step [576/782], Loss: 3.5067\n",
      "Epoch [1/50], Step [640/782], Loss: 3.5641\n",
      "Epoch [1/50], Step [704/782], Loss: 3.3963\n",
      "Epoch [1/50], Step [768/782], Loss: 3.6095\n",
      "Epoch [2/50], Step [64/782], Loss: 3.4573\n",
      "Epoch [2/50], Step [128/782], Loss: 3.1059\n",
      "Epoch [2/50], Step [192/782], Loss: 3.5098\n",
      "Epoch [2/50], Step [256/782], Loss: 2.9318\n",
      "Epoch [2/50], Step [320/782], Loss: 3.2298\n",
      "Epoch [2/50], Step [384/782], Loss: 3.0531\n",
      "Epoch [2/50], Step [448/782], Loss: 3.2214\n",
      "Epoch [2/50], Step [512/782], Loss: 2.9463\n",
      "Epoch [2/50], Step [576/782], Loss: 2.8269\n",
      "Epoch [2/50], Step [640/782], Loss: 2.9974\n",
      "Epoch [2/50], Step [704/782], Loss: 3.1770\n",
      "Epoch [2/50], Step [768/782], Loss: 2.8993\n",
      "Epoch [3/50], Step [64/782], Loss: 2.8381\n",
      "Epoch [3/50], Step [128/782], Loss: 2.7757\n",
      "Epoch [3/50], Step [192/782], Loss: 2.6403\n",
      "Epoch [3/50], Step [256/782], Loss: 2.7153\n",
      "Epoch [3/50], Step [320/782], Loss: 2.9218\n",
      "Epoch [3/50], Step [384/782], Loss: 2.7204\n",
      "Epoch [3/50], Step [448/782], Loss: 2.8673\n",
      "Epoch [3/50], Step [512/782], Loss: 2.8985\n",
      "Epoch [3/50], Step [576/782], Loss: 2.5790\n",
      "Epoch [3/50], Step [640/782], Loss: 2.5035\n",
      "Epoch [3/50], Step [704/782], Loss: 2.7524\n",
      "Epoch [3/50], Step [768/782], Loss: 2.4897\n",
      "Epoch [4/50], Step [64/782], Loss: 2.6782\n",
      "Epoch [4/50], Step [128/782], Loss: 2.8135\n",
      "Epoch [4/50], Step [192/782], Loss: 2.5209\n",
      "Epoch [4/50], Step [256/782], Loss: 2.5863\n",
      "Epoch [4/50], Step [320/782], Loss: 2.1308\n",
      "Epoch [4/50], Step [384/782], Loss: 2.5983\n",
      "Epoch [4/50], Step [448/782], Loss: 2.3122\n",
      "Epoch [4/50], Step [512/782], Loss: 3.3516\n",
      "Epoch [4/50], Step [576/782], Loss: 2.8582\n",
      "Epoch [4/50], Step [640/782], Loss: 2.5084\n",
      "Epoch [4/50], Step [704/782], Loss: 2.7382\n",
      "Epoch [4/50], Step [768/782], Loss: 2.7741\n",
      "Epoch [5/50], Step [64/782], Loss: 2.4667\n",
      "Epoch [5/50], Step [128/782], Loss: 2.2341\n",
      "Epoch [5/50], Step [192/782], Loss: 2.3402\n",
      "Epoch [5/50], Step [256/782], Loss: 2.4287\n",
      "Epoch [5/50], Step [320/782], Loss: 2.7719\n",
      "Epoch [5/50], Step [384/782], Loss: 2.1579\n",
      "Epoch [5/50], Step [448/782], Loss: 2.3608\n",
      "Epoch [5/50], Step [512/782], Loss: 2.5422\n",
      "Epoch [5/50], Step [576/782], Loss: 2.3801\n",
      "Epoch [5/50], Step [640/782], Loss: 2.6931\n",
      "Epoch [5/50], Step [704/782], Loss: 2.4171\n",
      "Epoch [5/50], Step [768/782], Loss: 2.5725\n",
      "Epoch [6/50], Step [64/782], Loss: 2.6528\n",
      "Epoch [6/50], Step [128/782], Loss: 1.9506\n",
      "Epoch [6/50], Step [192/782], Loss: 2.6640\n",
      "Epoch [6/50], Step [256/782], Loss: 2.2453\n",
      "Epoch [6/50], Step [320/782], Loss: 2.6419\n",
      "Epoch [6/50], Step [384/782], Loss: 2.5231\n",
      "Epoch [6/50], Step [448/782], Loss: 2.3985\n",
      "Epoch [6/50], Step [512/782], Loss: 2.3526\n",
      "Epoch [6/50], Step [576/782], Loss: 2.5171\n",
      "Epoch [6/50], Step [640/782], Loss: 2.2204\n",
      "Epoch [6/50], Step [704/782], Loss: 2.5748\n",
      "Epoch [6/50], Step [768/782], Loss: 2.3407\n",
      "Epoch [7/50], Step [64/782], Loss: 1.6751\n",
      "Epoch [7/50], Step [128/782], Loss: 1.7803\n",
      "Epoch [7/50], Step [192/782], Loss: 2.3448\n",
      "Epoch [7/50], Step [256/782], Loss: 1.8389\n",
      "Epoch [7/50], Step [320/782], Loss: 1.9970\n",
      "Epoch [7/50], Step [384/782], Loss: 2.0314\n",
      "Epoch [7/50], Step [448/782], Loss: 2.2997\n",
      "Epoch [7/50], Step [512/782], Loss: 1.7906\n",
      "Epoch [7/50], Step [576/782], Loss: 2.1531\n",
      "Epoch [7/50], Step [640/782], Loss: 1.8782\n",
      "Epoch [7/50], Step [704/782], Loss: 2.3380\n",
      "Epoch [7/50], Step [768/782], Loss: 2.1783\n",
      "Epoch [8/50], Step [64/782], Loss: 2.3481\n",
      "Epoch [8/50], Step [128/782], Loss: 1.6518\n",
      "Epoch [8/50], Step [192/782], Loss: 1.9703\n",
      "Epoch [8/50], Step [256/782], Loss: 1.7651\n",
      "Epoch [8/50], Step [320/782], Loss: 1.9054\n",
      "Epoch [8/50], Step [384/782], Loss: 1.9287\n",
      "Epoch [8/50], Step [448/782], Loss: 2.0864\n",
      "Epoch [8/50], Step [512/782], Loss: 2.1328\n",
      "Epoch [8/50], Step [576/782], Loss: 2.0078\n",
      "Epoch [8/50], Step [640/782], Loss: 2.2377\n",
      "Epoch [8/50], Step [704/782], Loss: 1.6967\n",
      "Epoch [8/50], Step [768/782], Loss: 2.2620\n",
      "Epoch [9/50], Step [64/782], Loss: 2.0997\n",
      "Epoch [9/50], Step [128/782], Loss: 2.0214\n",
      "Epoch [9/50], Step [192/782], Loss: 2.0399\n",
      "Epoch [9/50], Step [256/782], Loss: 1.9077\n",
      "Epoch [9/50], Step [320/782], Loss: 1.8684\n",
      "Epoch [9/50], Step [384/782], Loss: 1.8689\n",
      "Epoch [9/50], Step [448/782], Loss: 1.8536\n",
      "Epoch [9/50], Step [512/782], Loss: 1.8561\n",
      "Epoch [9/50], Step [576/782], Loss: 1.7649\n",
      "Epoch [9/50], Step [640/782], Loss: 2.2203\n",
      "Epoch [9/50], Step [704/782], Loss: 2.0734\n",
      "Epoch [9/50], Step [768/782], Loss: 2.0121\n",
      "Epoch [10/50], Step [64/782], Loss: 2.0237\n",
      "Epoch [10/50], Step [128/782], Loss: 1.7193\n",
      "Epoch [10/50], Step [192/782], Loss: 1.8916\n",
      "Epoch [10/50], Step [256/782], Loss: 1.9847\n",
      "Epoch [10/50], Step [320/782], Loss: 1.7345\n",
      "Epoch [10/50], Step [384/782], Loss: 1.6223\n",
      "Epoch [10/50], Step [448/782], Loss: 2.0352\n",
      "Epoch [10/50], Step [512/782], Loss: 1.9302\n",
      "Epoch [10/50], Step [576/782], Loss: 1.7474\n",
      "Epoch [10/50], Step [640/782], Loss: 1.7449\n",
      "Epoch [10/50], Step [704/782], Loss: 2.2251\n",
      "Epoch [10/50], Step [768/782], Loss: 2.1374\n",
      "Epoch [11/50], Step [64/782], Loss: 1.6234\n",
      "Epoch [11/50], Step [128/782], Loss: 1.3115\n",
      "Epoch [11/50], Step [192/782], Loss: 1.6307\n",
      "Epoch [11/50], Step [256/782], Loss: 1.5576\n",
      "Epoch [11/50], Step [320/782], Loss: 1.7934\n",
      "Epoch [11/50], Step [384/782], Loss: 1.2370\n",
      "Epoch [11/50], Step [448/782], Loss: 1.5238\n",
      "Epoch [11/50], Step [512/782], Loss: 1.4005\n",
      "Epoch [11/50], Step [576/782], Loss: 1.8927\n",
      "Epoch [11/50], Step [640/782], Loss: 1.5752\n",
      "Epoch [11/50], Step [704/782], Loss: 1.7274\n",
      "Epoch [11/50], Step [768/782], Loss: 1.4978\n",
      "Epoch [12/50], Step [64/782], Loss: 1.3882\n",
      "Epoch [12/50], Step [128/782], Loss: 1.4933\n",
      "Epoch [12/50], Step [192/782], Loss: 1.3603\n",
      "Epoch [12/50], Step [256/782], Loss: 1.2166\n",
      "Epoch [12/50], Step [320/782], Loss: 1.2301\n",
      "Epoch [12/50], Step [384/782], Loss: 1.8227\n",
      "Epoch [12/50], Step [448/782], Loss: 1.4982\n",
      "Epoch [12/50], Step [512/782], Loss: 1.3008\n",
      "Epoch [12/50], Step [576/782], Loss: 1.5327\n",
      "Epoch [12/50], Step [640/782], Loss: 1.3296\n",
      "Epoch [12/50], Step [704/782], Loss: 1.4427\n",
      "Epoch [12/50], Step [768/782], Loss: 1.5325\n",
      "Epoch [13/50], Step [64/782], Loss: 1.3164\n",
      "Epoch [13/50], Step [128/782], Loss: 1.5310\n",
      "Epoch [13/50], Step [192/782], Loss: 1.2174\n",
      "Epoch [13/50], Step [256/782], Loss: 1.5938\n",
      "Epoch [13/50], Step [320/782], Loss: 1.3441\n",
      "Epoch [13/50], Step [384/782], Loss: 1.3279\n",
      "Epoch [13/50], Step [448/782], Loss: 1.4690\n",
      "Epoch [13/50], Step [512/782], Loss: 1.3410\n",
      "Epoch [13/50], Step [576/782], Loss: 1.4326\n",
      "Epoch [13/50], Step [640/782], Loss: 1.5615\n",
      "Epoch [13/50], Step [704/782], Loss: 1.6552\n",
      "Epoch [13/50], Step [768/782], Loss: 1.1137\n",
      "Epoch [14/50], Step [64/782], Loss: 1.1179\n",
      "Epoch [14/50], Step [128/782], Loss: 1.1322\n",
      "Epoch [14/50], Step [192/782], Loss: 0.8897\n",
      "Epoch [14/50], Step [256/782], Loss: 0.7989\n",
      "Epoch [14/50], Step [320/782], Loss: 1.2268\n",
      "Epoch [14/50], Step [384/782], Loss: 1.3332\n",
      "Epoch [14/50], Step [448/782], Loss: 1.4564\n",
      "Epoch [14/50], Step [512/782], Loss: 1.2088\n",
      "Epoch [14/50], Step [576/782], Loss: 1.1071\n",
      "Epoch [14/50], Step [640/782], Loss: 1.4974\n",
      "Epoch [14/50], Step [704/782], Loss: 1.4686\n",
      "Epoch [14/50], Step [768/782], Loss: 1.5761\n",
      "Epoch [15/50], Step [64/782], Loss: 1.1986\n",
      "Epoch [15/50], Step [128/782], Loss: 1.0386\n",
      "Epoch [15/50], Step [192/782], Loss: 1.3529\n",
      "Epoch [15/50], Step [256/782], Loss: 1.2928\n",
      "Epoch [15/50], Step [320/782], Loss: 1.1321\n",
      "Epoch [15/50], Step [384/782], Loss: 0.8070\n",
      "Epoch [15/50], Step [448/782], Loss: 1.0815\n",
      "Epoch [15/50], Step [512/782], Loss: 1.0886\n",
      "Epoch [15/50], Step [576/782], Loss: 1.2568\n",
      "Epoch [15/50], Step [640/782], Loss: 1.1621\n",
      "Epoch [15/50], Step [704/782], Loss: 1.2006\n",
      "Epoch [15/50], Step [768/782], Loss: 1.0338\n",
      "Epoch [16/50], Step [64/782], Loss: 1.0849\n",
      "Epoch [16/50], Step [128/782], Loss: 0.9673\n",
      "Epoch [16/50], Step [192/782], Loss: 1.2275\n",
      "Epoch [16/50], Step [256/782], Loss: 0.9676\n",
      "Epoch [16/50], Step [320/782], Loss: 0.9920\n",
      "Epoch [16/50], Step [384/782], Loss: 1.0281\n",
      "Epoch [16/50], Step [448/782], Loss: 1.0039\n",
      "Epoch [16/50], Step [512/782], Loss: 1.2403\n",
      "Epoch [16/50], Step [576/782], Loss: 1.0450\n",
      "Epoch [16/50], Step [640/782], Loss: 1.3418\n",
      "Epoch [16/50], Step [704/782], Loss: 1.2069\n",
      "Epoch [16/50], Step [768/782], Loss: 1.0506\n",
      "Epoch [17/50], Step [64/782], Loss: 0.8600\n",
      "Epoch [17/50], Step [128/782], Loss: 0.7294\n",
      "Epoch [17/50], Step [192/782], Loss: 1.0054\n",
      "Epoch [17/50], Step [256/782], Loss: 0.9582\n",
      "Epoch [17/50], Step [320/782], Loss: 0.7211\n",
      "Epoch [17/50], Step [384/782], Loss: 1.0387\n",
      "Epoch [17/50], Step [448/782], Loss: 0.8344\n",
      "Epoch [17/50], Step [512/782], Loss: 1.1728\n",
      "Epoch [17/50], Step [576/782], Loss: 0.8181\n",
      "Epoch [17/50], Step [640/782], Loss: 0.9771\n",
      "Epoch [17/50], Step [704/782], Loss: 1.0274\n",
      "Epoch [17/50], Step [768/782], Loss: 1.4550\n",
      "Epoch [18/50], Step [64/782], Loss: 1.0167\n",
      "Epoch [18/50], Step [128/782], Loss: 0.6657\n",
      "Epoch [18/50], Step [192/782], Loss: 0.6003\n",
      "Epoch [18/50], Step [256/782], Loss: 1.2048\n",
      "Epoch [18/50], Step [320/782], Loss: 0.6650\n",
      "Epoch [18/50], Step [384/782], Loss: 0.7922\n",
      "Epoch [18/50], Step [448/782], Loss: 0.6717\n",
      "Epoch [18/50], Step [512/782], Loss: 1.0726\n",
      "Epoch [18/50], Step [576/782], Loss: 0.8518\n",
      "Epoch [18/50], Step [640/782], Loss: 1.1165\n",
      "Epoch [18/50], Step [704/782], Loss: 1.2052\n",
      "Epoch [18/50], Step [768/782], Loss: 0.9810\n",
      "Epoch [19/50], Step [64/782], Loss: 0.5348\n",
      "Epoch [19/50], Step [128/782], Loss: 0.6027\n",
      "Epoch [19/50], Step [192/782], Loss: 0.5392\n",
      "Epoch [19/50], Step [256/782], Loss: 0.7320\n",
      "Epoch [19/50], Step [320/782], Loss: 0.7434\n",
      "Epoch [19/50], Step [384/782], Loss: 0.8689\n",
      "Epoch [19/50], Step [448/782], Loss: 0.8372\n",
      "Epoch [19/50], Step [512/782], Loss: 0.7323\n",
      "Epoch [19/50], Step [576/782], Loss: 0.6694\n",
      "Epoch [19/50], Step [640/782], Loss: 1.2317\n",
      "Epoch [19/50], Step [704/782], Loss: 0.7767\n",
      "Epoch [19/50], Step [768/782], Loss: 1.1720\n",
      "Epoch [20/50], Step [64/782], Loss: 0.6139\n",
      "Epoch [20/50], Step [128/782], Loss: 0.5390\n",
      "Epoch [20/50], Step [192/782], Loss: 0.8505\n",
      "Epoch [20/50], Step [256/782], Loss: 0.6966\n",
      "Epoch [20/50], Step [320/782], Loss: 0.5612\n",
      "Epoch [20/50], Step [384/782], Loss: 0.5476\n",
      "Epoch [20/50], Step [448/782], Loss: 0.9850\n",
      "Epoch [20/50], Step [512/782], Loss: 0.6997\n",
      "Epoch [20/50], Step [576/782], Loss: 0.7523\n",
      "Epoch [20/50], Step [640/782], Loss: 1.0974\n",
      "Epoch [20/50], Step [704/782], Loss: 0.6682\n",
      "Epoch [20/50], Step [768/782], Loss: 0.8429\n",
      "Epoch [21/50], Step [64/782], Loss: 0.6265\n",
      "Epoch [21/50], Step [128/782], Loss: 0.5623\n",
      "Epoch [21/50], Step [192/782], Loss: 0.5978\n",
      "Epoch [21/50], Step [256/782], Loss: 0.6306\n",
      "Epoch [21/50], Step [320/782], Loss: 0.7437\n",
      "Epoch [21/50], Step [384/782], Loss: 0.7068\n",
      "Epoch [21/50], Step [448/782], Loss: 0.5788\n",
      "Epoch [21/50], Step [512/782], Loss: 0.8379\n",
      "Epoch [21/50], Step [576/782], Loss: 1.0816\n",
      "Epoch [21/50], Step [640/782], Loss: 0.9296\n",
      "Epoch [21/50], Step [704/782], Loss: 0.7614\n",
      "Epoch [21/50], Step [768/782], Loss: 0.8104\n",
      "Epoch [22/50], Step [64/782], Loss: 0.3854\n",
      "Epoch [22/50], Step [128/782], Loss: 0.3328\n",
      "Epoch [22/50], Step [192/782], Loss: 0.5823\n",
      "Epoch [22/50], Step [256/782], Loss: 0.8091\n",
      "Epoch [22/50], Step [320/782], Loss: 0.7865\n",
      "Epoch [22/50], Step [384/782], Loss: 0.4984\n",
      "Epoch [22/50], Step [448/782], Loss: 0.7069\n",
      "Epoch [22/50], Step [512/782], Loss: 0.7630\n",
      "Epoch [22/50], Step [576/782], Loss: 0.6215\n",
      "Epoch [22/50], Step [640/782], Loss: 0.6193\n",
      "Epoch [22/50], Step [704/782], Loss: 1.0115\n",
      "Epoch [22/50], Step [768/782], Loss: 0.6790\n",
      "Epoch [23/50], Step [64/782], Loss: 0.4231\n",
      "Epoch [23/50], Step [128/782], Loss: 0.6240\n",
      "Epoch [23/50], Step [192/782], Loss: 0.5333\n",
      "Epoch [23/50], Step [256/782], Loss: 0.3625\n",
      "Epoch [23/50], Step [320/782], Loss: 0.4999\n",
      "Epoch [23/50], Step [384/782], Loss: 0.6388\n",
      "Epoch [23/50], Step [448/782], Loss: 0.6233\n",
      "Epoch [23/50], Step [512/782], Loss: 1.0220\n",
      "Epoch [23/50], Step [576/782], Loss: 0.5300\n",
      "Epoch [23/50], Step [640/782], Loss: 0.7921\n",
      "Epoch [23/50], Step [704/782], Loss: 0.9084\n",
      "Epoch [23/50], Step [768/782], Loss: 0.5551\n",
      "Epoch [24/50], Step [64/782], Loss: 0.2708\n",
      "Epoch [24/50], Step [128/782], Loss: 0.3809\n",
      "Epoch [24/50], Step [192/782], Loss: 0.3789\n",
      "Epoch [24/50], Step [256/782], Loss: 0.6862\n",
      "Epoch [24/50], Step [320/782], Loss: 0.5048\n",
      "Epoch [24/50], Step [384/782], Loss: 0.6624\n",
      "Epoch [24/50], Step [448/782], Loss: 0.7975\n",
      "Epoch [24/50], Step [512/782], Loss: 0.6245\n",
      "Epoch [24/50], Step [576/782], Loss: 0.7646\n",
      "Epoch [24/50], Step [640/782], Loss: 0.6621\n",
      "Epoch [24/50], Step [704/782], Loss: 0.7165\n",
      "Epoch [24/50], Step [768/782], Loss: 0.6717\n",
      "Epoch [25/50], Step [64/782], Loss: 0.4821\n",
      "Epoch [25/50], Step [128/782], Loss: 0.4993\n",
      "Epoch [25/50], Step [192/782], Loss: 0.6336\n",
      "Epoch [25/50], Step [256/782], Loss: 0.5193\n",
      "Epoch [25/50], Step [320/782], Loss: 0.6661\n",
      "Epoch [25/50], Step [384/782], Loss: 0.6759\n",
      "Epoch [25/50], Step [448/782], Loss: 0.3645\n",
      "Epoch [25/50], Step [512/782], Loss: 0.5749\n",
      "Epoch [25/50], Step [576/782], Loss: 0.4451\n",
      "Epoch [25/50], Step [640/782], Loss: 0.4762\n",
      "Epoch [25/50], Step [704/782], Loss: 0.6765\n",
      "Epoch [25/50], Step [768/782], Loss: 0.8759\n",
      "Epoch [26/50], Step [64/782], Loss: 0.3833\n",
      "Epoch [26/50], Step [128/782], Loss: 0.3219\n",
      "Epoch [26/50], Step [192/782], Loss: 0.2415\n",
      "Epoch [26/50], Step [256/782], Loss: 0.4609\n",
      "Epoch [26/50], Step [320/782], Loss: 0.4281\n",
      "Epoch [26/50], Step [384/782], Loss: 0.4617\n",
      "Epoch [26/50], Step [448/782], Loss: 0.7814\n",
      "Epoch [26/50], Step [512/782], Loss: 0.5190\n",
      "Epoch [26/50], Step [576/782], Loss: 0.6220\n",
      "Epoch [26/50], Step [640/782], Loss: 0.5305\n",
      "Epoch [26/50], Step [704/782], Loss: 0.6479\n",
      "Epoch [26/50], Step [768/782], Loss: 0.5220\n",
      "Epoch [27/50], Step [64/782], Loss: 0.2942\n",
      "Epoch [27/50], Step [128/782], Loss: 0.2735\n",
      "Epoch [27/50], Step [192/782], Loss: 0.3676\n",
      "Epoch [27/50], Step [256/782], Loss: 0.3011\n",
      "Epoch [27/50], Step [320/782], Loss: 0.4748\n",
      "Epoch [27/50], Step [384/782], Loss: 0.5823\n",
      "Epoch [27/50], Step [448/782], Loss: 0.3930\n",
      "Epoch [27/50], Step [512/782], Loss: 0.4408\n",
      "Epoch [27/50], Step [576/782], Loss: 0.6398\n",
      "Epoch [27/50], Step [640/782], Loss: 0.6587\n",
      "Epoch [27/50], Step [704/782], Loss: 0.4121\n",
      "Epoch [27/50], Step [768/782], Loss: 0.6399\n",
      "Epoch [28/50], Step [64/782], Loss: 0.3211\n",
      "Epoch [28/50], Step [128/782], Loss: 0.2891\n",
      "Epoch [28/50], Step [192/782], Loss: 0.1760\n",
      "Epoch [28/50], Step [256/782], Loss: 0.2248\n",
      "Epoch [28/50], Step [320/782], Loss: 0.4003\n",
      "Epoch [28/50], Step [384/782], Loss: 0.3007\n",
      "Epoch [28/50], Step [448/782], Loss: 0.3529\n",
      "Epoch [28/50], Step [512/782], Loss: 0.6025\n",
      "Epoch [28/50], Step [576/782], Loss: 0.4733\n",
      "Epoch [28/50], Step [640/782], Loss: 0.4609\n",
      "Epoch [28/50], Step [704/782], Loss: 0.7567\n",
      "Epoch [28/50], Step [768/782], Loss: 0.5077\n",
      "Epoch [29/50], Step [64/782], Loss: 0.3573\n",
      "Epoch [29/50], Step [128/782], Loss: 0.3510\n",
      "Epoch [29/50], Step [192/782], Loss: 0.4585\n",
      "Epoch [29/50], Step [256/782], Loss: 0.1960\n",
      "Epoch [29/50], Step [320/782], Loss: 0.3536\n",
      "Epoch [29/50], Step [384/782], Loss: 0.4108\n",
      "Epoch [29/50], Step [448/782], Loss: 0.4270\n",
      "Epoch [29/50], Step [512/782], Loss: 0.4833\n",
      "Epoch [29/50], Step [576/782], Loss: 0.4216\n",
      "Epoch [29/50], Step [640/782], Loss: 0.2901\n",
      "Epoch [29/50], Step [704/782], Loss: 0.7734\n",
      "Epoch [29/50], Step [768/782], Loss: 0.4694\n",
      "Epoch [30/50], Step [64/782], Loss: 0.4049\n",
      "Epoch [30/50], Step [128/782], Loss: 0.3179\n",
      "Epoch [30/50], Step [192/782], Loss: 0.5347\n",
      "Epoch [30/50], Step [256/782], Loss: 0.4343\n",
      "Epoch [30/50], Step [320/782], Loss: 0.3740\n",
      "Epoch [30/50], Step [384/782], Loss: 0.3868\n",
      "Epoch [30/50], Step [448/782], Loss: 0.4730\n",
      "Epoch [30/50], Step [512/782], Loss: 0.5776\n",
      "Epoch [30/50], Step [576/782], Loss: 0.4316\n",
      "Epoch [30/50], Step [640/782], Loss: 0.4796\n",
      "Epoch [30/50], Step [704/782], Loss: 0.3515\n",
      "Epoch [30/50], Step [768/782], Loss: 0.3856\n",
      "Epoch [31/50], Step [64/782], Loss: 0.3257\n",
      "Epoch [31/50], Step [128/782], Loss: 0.3846\n",
      "Epoch [31/50], Step [192/782], Loss: 0.7006\n",
      "Epoch [31/50], Step [256/782], Loss: 0.1953\n",
      "Epoch [31/50], Step [320/782], Loss: 0.5083\n",
      "Epoch [31/50], Step [384/782], Loss: 0.4054\n",
      "Epoch [31/50], Step [448/782], Loss: 0.2252\n",
      "Epoch [31/50], Step [512/782], Loss: 0.4338\n",
      "Epoch [31/50], Step [576/782], Loss: 0.4086\n",
      "Epoch [31/50], Step [640/782], Loss: 0.2756\n",
      "Epoch [31/50], Step [704/782], Loss: 0.5099\n",
      "Epoch [31/50], Step [768/782], Loss: 0.6355\n",
      "Epoch [32/50], Step [64/782], Loss: 0.2903\n",
      "Epoch [32/50], Step [128/782], Loss: 0.2493\n",
      "Epoch [32/50], Step [192/782], Loss: 0.4337\n",
      "Epoch [32/50], Step [256/782], Loss: 0.3760\n",
      "Epoch [32/50], Step [320/782], Loss: 0.3048\n",
      "Epoch [32/50], Step [384/782], Loss: 0.5129\n",
      "Epoch [32/50], Step [448/782], Loss: 0.2555\n",
      "Epoch [32/50], Step [512/782], Loss: 0.4972\n",
      "Epoch [32/50], Step [576/782], Loss: 0.7041\n",
      "Epoch [32/50], Step [640/782], Loss: 0.3921\n",
      "Epoch [32/50], Step [704/782], Loss: 0.4956\n",
      "Epoch [32/50], Step [768/782], Loss: 0.4628\n",
      "Epoch [33/50], Step [64/782], Loss: 0.3280\n",
      "Epoch [33/50], Step [128/782], Loss: 0.1239\n",
      "Epoch [33/50], Step [192/782], Loss: 0.1963\n",
      "Epoch [33/50], Step [256/782], Loss: 0.2384\n",
      "Epoch [33/50], Step [320/782], Loss: 0.5360\n",
      "Epoch [33/50], Step [384/782], Loss: 0.2093\n",
      "Epoch [33/50], Step [448/782], Loss: 0.4639\n",
      "Epoch [33/50], Step [512/782], Loss: 0.5465\n",
      "Epoch [33/50], Step [576/782], Loss: 0.2426\n",
      "Epoch [33/50], Step [640/782], Loss: 0.6183\n",
      "Epoch [33/50], Step [704/782], Loss: 0.4734\n",
      "Epoch [33/50], Step [768/782], Loss: 0.3039\n",
      "Epoch [34/50], Step [64/782], Loss: 0.3975\n",
      "Epoch [34/50], Step [128/782], Loss: 0.1395\n",
      "Epoch [34/50], Step [192/782], Loss: 0.2208\n",
      "Epoch [34/50], Step [256/782], Loss: 0.2752\n",
      "Epoch [34/50], Step [320/782], Loss: 0.2515\n",
      "Epoch [34/50], Step [384/782], Loss: 0.3238\n",
      "Epoch [34/50], Step [448/782], Loss: 0.2589\n",
      "Epoch [34/50], Step [512/782], Loss: 0.4525\n",
      "Epoch [34/50], Step [576/782], Loss: 0.5251\n",
      "Epoch [34/50], Step [640/782], Loss: 0.4280\n",
      "Epoch [34/50], Step [704/782], Loss: 0.5006\n",
      "Epoch [34/50], Step [768/782], Loss: 0.3739\n",
      "Epoch [35/50], Step [64/782], Loss: 0.2976\n",
      "Epoch [35/50], Step [128/782], Loss: 0.2498\n",
      "Epoch [35/50], Step [192/782], Loss: 0.1399\n",
      "Epoch [35/50], Step [256/782], Loss: 0.2421\n",
      "Epoch [35/50], Step [320/782], Loss: 0.2651\n",
      "Epoch [35/50], Step [384/782], Loss: 0.2420\n",
      "Epoch [35/50], Step [448/782], Loss: 0.3148\n",
      "Epoch [35/50], Step [512/782], Loss: 0.4511\n",
      "Epoch [35/50], Step [576/782], Loss: 0.3501\n",
      "Epoch [35/50], Step [640/782], Loss: 0.5448\n",
      "Epoch [35/50], Step [704/782], Loss: 0.2809\n",
      "Epoch [35/50], Step [768/782], Loss: 0.3658\n",
      "Epoch [36/50], Step [64/782], Loss: 0.3290\n",
      "Epoch [36/50], Step [128/782], Loss: 0.3715\n",
      "Epoch [36/50], Step [192/782], Loss: 0.3507\n",
      "Epoch [36/50], Step [256/782], Loss: 0.2398\n",
      "Epoch [36/50], Step [320/782], Loss: 0.2675\n",
      "Epoch [36/50], Step [384/782], Loss: 0.3539\n",
      "Epoch [36/50], Step [448/782], Loss: 0.4930\n",
      "Epoch [36/50], Step [512/782], Loss: 0.1755\n",
      "Epoch [36/50], Step [576/782], Loss: 0.4471\n",
      "Epoch [36/50], Step [640/782], Loss: 0.7805\n",
      "Epoch [36/50], Step [704/782], Loss: 0.5159\n",
      "Epoch [36/50], Step [768/782], Loss: 0.4816\n",
      "Epoch [37/50], Step [64/782], Loss: 0.3313\n",
      "Epoch [37/50], Step [128/782], Loss: 0.3500\n",
      "Epoch [37/50], Step [192/782], Loss: 0.2658\n",
      "Epoch [37/50], Step [256/782], Loss: 0.3183\n",
      "Epoch [37/50], Step [320/782], Loss: 0.6074\n",
      "Epoch [37/50], Step [384/782], Loss: 0.3105\n",
      "Epoch [37/50], Step [448/782], Loss: 0.3240\n",
      "Epoch [37/50], Step [512/782], Loss: 0.4719\n",
      "Epoch [37/50], Step [576/782], Loss: 0.4509\n",
      "Epoch [37/50], Step [640/782], Loss: 0.3832\n",
      "Epoch [37/50], Step [704/782], Loss: 0.4803\n",
      "Epoch [37/50], Step [768/782], Loss: 0.5136\n",
      "Epoch [38/50], Step [64/782], Loss: 0.5070\n",
      "Epoch [38/50], Step [128/782], Loss: 0.1979\n",
      "Epoch [38/50], Step [192/782], Loss: 0.2639\n",
      "Epoch [38/50], Step [256/782], Loss: 0.2597\n",
      "Epoch [38/50], Step [320/782], Loss: 0.2215\n",
      "Epoch [38/50], Step [384/782], Loss: 0.3197\n",
      "Epoch [38/50], Step [448/782], Loss: 0.3386\n",
      "Epoch [38/50], Step [512/782], Loss: 0.4155\n",
      "Epoch [38/50], Step [576/782], Loss: 0.5560\n",
      "Epoch [38/50], Step [640/782], Loss: 0.5025\n",
      "Epoch [38/50], Step [704/782], Loss: 0.3501\n",
      "Epoch [38/50], Step [768/782], Loss: 0.7981\n",
      "Epoch [39/50], Step [64/782], Loss: 0.1231\n",
      "Epoch [39/50], Step [128/782], Loss: 0.2087\n",
      "Epoch [39/50], Step [192/782], Loss: 0.2746\n",
      "Epoch [39/50], Step [256/782], Loss: 0.2751\n",
      "Epoch [39/50], Step [320/782], Loss: 0.1295\n",
      "Epoch [39/50], Step [384/782], Loss: 0.2465\n",
      "Epoch [39/50], Step [448/782], Loss: 0.1726\n",
      "Epoch [39/50], Step [512/782], Loss: 0.2096\n",
      "Epoch [39/50], Step [576/782], Loss: 0.5790\n",
      "Epoch [39/50], Step [640/782], Loss: 0.1963\n",
      "Epoch [39/50], Step [704/782], Loss: 0.4600\n",
      "Epoch [39/50], Step [768/782], Loss: 0.3919\n",
      "Epoch [40/50], Step [64/782], Loss: 0.2017\n",
      "Epoch [40/50], Step [128/782], Loss: 0.1655\n",
      "Epoch [40/50], Step [192/782], Loss: 0.3459\n",
      "Epoch [40/50], Step [256/782], Loss: 0.3311\n",
      "Epoch [40/50], Step [320/782], Loss: 0.2310\n",
      "Epoch [40/50], Step [384/782], Loss: 0.2308\n",
      "Epoch [40/50], Step [448/782], Loss: 0.2235\n",
      "Epoch [40/50], Step [512/782], Loss: 0.5181\n",
      "Epoch [40/50], Step [576/782], Loss: 0.3127\n",
      "Epoch [40/50], Step [640/782], Loss: 0.4294\n",
      "Epoch [40/50], Step [704/782], Loss: 0.4191\n",
      "Epoch [40/50], Step [768/782], Loss: 0.3813\n",
      "Epoch [41/50], Step [64/782], Loss: 0.2102\n",
      "Epoch [41/50], Step [128/782], Loss: 0.2697\n",
      "Epoch [41/50], Step [192/782], Loss: 0.3667\n",
      "Epoch [41/50], Step [256/782], Loss: 0.1900\n",
      "Epoch [41/50], Step [320/782], Loss: 0.3053\n",
      "Epoch [41/50], Step [384/782], Loss: 0.4606\n",
      "Epoch [41/50], Step [448/782], Loss: 0.4593\n",
      "Epoch [41/50], Step [512/782], Loss: 0.5242\n",
      "Epoch [41/50], Step [576/782], Loss: 0.5432\n",
      "Epoch [41/50], Step [640/782], Loss: 0.4078\n",
      "Epoch [41/50], Step [704/782], Loss: 0.2005\n",
      "Epoch [41/50], Step [768/782], Loss: 0.2807\n",
      "Epoch [42/50], Step [64/782], Loss: 0.4494\n",
      "Epoch [42/50], Step [128/782], Loss: 0.3916\n",
      "Epoch [42/50], Step [192/782], Loss: 0.1805\n",
      "Epoch [42/50], Step [256/782], Loss: 0.1911\n",
      "Epoch [42/50], Step [320/782], Loss: 0.4599\n",
      "Epoch [42/50], Step [384/782], Loss: 0.2581\n",
      "Epoch [42/50], Step [448/782], Loss: 0.0866\n",
      "Epoch [42/50], Step [512/782], Loss: 0.3904\n",
      "Epoch [42/50], Step [576/782], Loss: 0.2889\n",
      "Epoch [42/50], Step [640/782], Loss: 0.3632\n",
      "Epoch [42/50], Step [704/782], Loss: 0.3069\n",
      "Epoch [42/50], Step [768/782], Loss: 0.3457\n",
      "Epoch [43/50], Step [64/782], Loss: 0.2206\n",
      "Epoch [43/50], Step [128/782], Loss: 0.2185\n",
      "Epoch [43/50], Step [192/782], Loss: 0.1110\n",
      "Epoch [43/50], Step [256/782], Loss: 0.1505\n",
      "Epoch [43/50], Step [320/782], Loss: 0.4327\n",
      "Epoch [43/50], Step [384/782], Loss: 0.3538\n",
      "Epoch [43/50], Step [448/782], Loss: 0.4306\n",
      "Epoch [43/50], Step [512/782], Loss: 0.2958\n",
      "Epoch [43/50], Step [576/782], Loss: 0.2829\n",
      "Epoch [43/50], Step [640/782], Loss: 0.1913\n",
      "Epoch [43/50], Step [704/782], Loss: 0.4204\n",
      "Epoch [43/50], Step [768/782], Loss: 0.4117\n",
      "Epoch [44/50], Step [64/782], Loss: 0.1768\n",
      "Epoch [44/50], Step [128/782], Loss: 0.5172\n",
      "Epoch [44/50], Step [192/782], Loss: 0.1100\n",
      "Epoch [44/50], Step [256/782], Loss: 0.1524\n",
      "Epoch [44/50], Step [320/782], Loss: 0.2671\n",
      "Epoch [44/50], Step [384/782], Loss: 0.2583\n",
      "Epoch [44/50], Step [448/782], Loss: 0.4335\n",
      "Epoch [44/50], Step [512/782], Loss: 0.3551\n",
      "Epoch [44/50], Step [576/782], Loss: 0.6530\n",
      "Epoch [44/50], Step [640/782], Loss: 0.3595\n",
      "Epoch [44/50], Step [704/782], Loss: 0.7145\n",
      "Epoch [44/50], Step [768/782], Loss: 0.4346\n",
      "Epoch [45/50], Step [64/782], Loss: 0.2115\n",
      "Epoch [45/50], Step [128/782], Loss: 0.1878\n",
      "Epoch [45/50], Step [192/782], Loss: 0.1132\n",
      "Epoch [45/50], Step [256/782], Loss: 0.0631\n",
      "Epoch [45/50], Step [320/782], Loss: 0.2079\n",
      "Epoch [45/50], Step [384/782], Loss: 0.1526\n",
      "Epoch [45/50], Step [448/782], Loss: 0.2641\n",
      "Epoch [45/50], Step [512/782], Loss: 0.2785\n",
      "Epoch [45/50], Step [576/782], Loss: 0.5569\n",
      "Epoch [45/50], Step [640/782], Loss: 0.5534\n",
      "Epoch [45/50], Step [704/782], Loss: 0.4550\n",
      "Epoch [45/50], Step [768/782], Loss: 0.5015\n",
      "Epoch [46/50], Step [64/782], Loss: 0.3225\n",
      "Epoch [46/50], Step [128/782], Loss: 0.4357\n",
      "Epoch [46/50], Step [192/782], Loss: 0.2511\n",
      "Epoch [46/50], Step [256/782], Loss: 0.2083\n",
      "Epoch [46/50], Step [320/782], Loss: 0.3123\n",
      "Epoch [46/50], Step [384/782], Loss: 0.3241\n",
      "Epoch [46/50], Step [448/782], Loss: 0.2475\n",
      "Epoch [46/50], Step [512/782], Loss: 0.3124\n",
      "Epoch [46/50], Step [576/782], Loss: 0.2435\n",
      "Epoch [46/50], Step [640/782], Loss: 0.4466\n",
      "Epoch [46/50], Step [704/782], Loss: 0.5330\n",
      "Epoch [46/50], Step [768/782], Loss: 0.3169\n",
      "Epoch [47/50], Step [64/782], Loss: 0.2185\n",
      "Epoch [47/50], Step [128/782], Loss: 0.1576\n",
      "Epoch [47/50], Step [192/782], Loss: 0.2130\n",
      "Epoch [47/50], Step [256/782], Loss: 0.4461\n",
      "Epoch [47/50], Step [320/782], Loss: 0.2929\n",
      "Epoch [47/50], Step [384/782], Loss: 0.1749\n",
      "Epoch [47/50], Step [448/782], Loss: 0.3494\n",
      "Epoch [47/50], Step [512/782], Loss: 0.2563\n",
      "Epoch [47/50], Step [576/782], Loss: 0.5335\n",
      "Epoch [47/50], Step [640/782], Loss: 0.4077\n",
      "Epoch [47/50], Step [704/782], Loss: 0.4600\n",
      "Epoch [47/50], Step [768/782], Loss: 0.4976\n",
      "Epoch [48/50], Step [64/782], Loss: 0.1316\n",
      "Epoch [48/50], Step [128/782], Loss: 0.2924\n",
      "Epoch [48/50], Step [192/782], Loss: 0.1912\n",
      "Epoch [48/50], Step [256/782], Loss: 0.1273\n",
      "Epoch [48/50], Step [320/782], Loss: 0.1663\n",
      "Epoch [48/50], Step [384/782], Loss: 0.2880\n",
      "Epoch [48/50], Step [448/782], Loss: 0.2381\n",
      "Epoch [48/50], Step [512/782], Loss: 0.4461\n",
      "Epoch [48/50], Step [576/782], Loss: 0.5246\n",
      "Epoch [48/50], Step [640/782], Loss: 0.2437\n",
      "Epoch [48/50], Step [704/782], Loss: 0.3354\n",
      "Epoch [48/50], Step [768/782], Loss: 0.3843\n",
      "Epoch [49/50], Step [64/782], Loss: 0.1098\n",
      "Epoch [49/50], Step [128/782], Loss: 0.1565\n",
      "Epoch [49/50], Step [192/782], Loss: 0.1401\n",
      "Epoch [49/50], Step [256/782], Loss: 0.2752\n",
      "Epoch [49/50], Step [320/782], Loss: 0.2435\n",
      "Epoch [49/50], Step [384/782], Loss: 0.3463\n",
      "Epoch [49/50], Step [448/782], Loss: 0.3903\n",
      "Epoch [49/50], Step [512/782], Loss: 0.2287\n",
      "Epoch [49/50], Step [576/782], Loss: 0.1128\n",
      "Epoch [49/50], Step [640/782], Loss: 0.2814\n",
      "Epoch [49/50], Step [704/782], Loss: 0.2529\n",
      "Epoch [49/50], Step [768/782], Loss: 0.3721\n",
      "Epoch [50/50], Step [64/782], Loss: 0.0899\n",
      "Epoch [50/50], Step [128/782], Loss: 0.1838\n",
      "Epoch [50/50], Step [192/782], Loss: 0.2474\n",
      "Epoch [50/50], Step [256/782], Loss: 0.2513\n",
      "Epoch [50/50], Step [320/782], Loss: 0.3170\n",
      "Epoch [50/50], Step [384/782], Loss: 0.2642\n",
      "Epoch [50/50], Step [448/782], Loss: 0.2384\n",
      "Epoch [50/50], Step [512/782], Loss: 0.1318\n",
      "Epoch [50/50], Step [576/782], Loss: 0.3926\n",
      "Epoch [50/50], Step [640/782], Loss: 0.1589\n",
      "Epoch [50/50], Step [704/782], Loss: 0.1824\n",
      "Epoch [50/50], Step [768/782], Loss: 0.4368\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 64 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 27.77 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(100)]\n",
    "    n_class_samples = [0 for i in range(100)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        batch_size = labels.size(0)\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daoxuanbac/anaconda3/envs/pytorch_m1/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/daoxuanbac/anaconda3/envs/pytorch_m1/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
